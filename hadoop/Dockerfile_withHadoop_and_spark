##################################################################
#                   PROPERTY OF                                  #
#    Enigma Incorporated limited. Calcutta- West Bengal          #
#    Project    : INTEGRA                                        #
#    Copyrights : Closed source                                  #
#    Verion     : 2023.1.0.0                                     #
#    Author     : Chandan Kumar Goswami                          #
#    File       : Dockerfile_hadoop_spark_base                   #
#    About      : This docker file generates base image for Yarn #
#                Cluster for Apache spark project INTEGRA .      #
#                Following are the layers in the Docker file     #
#                1. ubuntu latest version                        #
#                2. Make machine ssh compatible ( passwd less )  #
#                3. Install basic sw vim, netools , python, pyth-#
#                   on venv etc.                                 #
#                                                                #
##################################################################

FROM ubuntu:latest

# Installtion of basic software
RUN apt-get update && apt-get install -y \
    openjdk-8-jdk \
    openssh-server \
    vim \
    net-tools \
    python3.10 \
    python3.10-venv \
    wget \
    && apt-get clean 

RUN  ln -s /usr/bin/python3 /usr/bin/python

# Configure SSH for Password less Login 
RUN mkdir /var/run/sshd && \
    sed -ri 's/^#?PermitRootLogin\s+.*/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -ri 's/^#?PasswordAuthentication\s+.*/PasswordAuthentication no/' /etc/ssh/sshd_config && \
    sed -ri 's/^#?ChallengeResponseAuthentication\s+.*/ChallengeResponseAuthentication no/' /etc/ssh/sshd_config

# Create hadoop user ( service acct. for non-root user )
RUN useradd -ms /bin/bash hadoop

RUN echo "root:Aubain1" | chpasswd 

# Pull hadoop binaries and install in home folder 
COPY SW_TAR/hadoop-3.2.2.tar.gz  /home/hadoop/

#RUN wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz 

RUN tar -xzf /home/hadoop/hadoop-3.2.2.tar.gz -C /home/hadoop 

RUN rm  /home/hadoop/hadoop-3.2.2.tar.gz

# Configure passWd less login
RUN mkdir -p /home/hadoop/.ssh && \
    ssh-keygen -f /home/hadoop/.ssh/id_rsa -N '' && \
    cp /home/hadoop/.ssh/id_rsa.pub /home/hadoop/.ssh/authorized_keys && \
    echo "Host * " > /home/hadoop/.ssh/config && \
    echo "    StrictHostKeyChecking no " >> /home/hadoop/.ssh/config && \
    chmod -R 755 /home/hadoop && \
    chmod 750 /home/hadoop/.ssh && \
    chmod 600 /home/hadoop/.ssh/id_rsa 
    
# Configure Hadoop environment variables
ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME /home/hadoop/hadoop-3.2.2
ENV PATH $PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
ENV HADOOP_CONF_DIR $HADOOP_HOME/etc/hadoop
ENV HDFS_NAMENODE_USER hadoop
ENV HDFS_DATANODE_USER hadoop
ENV HDFS_SECONDARYNAMENODE_USER hadoop
ENV YARN_RESOURCEMANAGER_USER hadoop
ENV YARN_NODEMANAGER_USER hadoop

# Copy Hadoop configuration files
COPY config/core-site.xml $HADOOP_CONF_DIR/
COPY config/hdfs-site.xml $HADOOP_CONF_DIR/
COPY config/mapred-site.xml $HADOOP_CONF_DIR/
COPY config/yarn-site.xml $HADOOP_CONF_DIR/
COPY config/workers $HADOOP_CONF_DIR/
COPY config/hadoop-env.sh $HADOOP_CONF_DIR/
COPY config/.bashrc /home/hadoop/
COPY config/hd-*.sh $HADOOP_HOME/sbin

# HDFS data Dir
RUN mkdir -p $HADOOP_CONF_DIR/data/nameNode
RUN mkdir -p $HADOOP_CONF_DIR/data/dataNode

# Expose Hadoop ports
EXPOSE 8080 7077 4040 9000 9870 8088


# SPARK INSTALLATION 
ENV SPARK_HOME /home/hadoop/spark-3.4.1

COPY SW_TAR/spark-3.4.1-bin-hadoop3-scala2.13.tgz  /home/hadoop/

RUN tar -xzf /home/hadoop/spark-3.4.1-bin-hadoop3-scala2.13.tgz  -C  /home/hadoop/

RUN mv /home/hadoop/spark-3.4.1-bin-hadoop3-scala2.13 $SPARK_HOME

RUN rm /home/hadoop/spark-3.4.1-bin-hadoop3-scala2.13.tgz

COPY config/spark-env.sh  $SPARK_HOME/conf
COPY config/spark-defaults.conf $SPARK_HOME/conf
COPY config/workers $SPARK_HOME/conf
COPY config/sp-*.sh $SPARK_HOME/sbin
COPY config/cluster_st* /home/hadoop

RUN mkdir  $SPARK_HOME/work

RUN chown -R hadoop:hadoop /home/hadoop

# Set the entry point to start the SSH server
CMD ["/usr/sbin/sshd", "-D"]
